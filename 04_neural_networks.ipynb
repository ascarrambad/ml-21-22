{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ascarrambad/ml-21-22/blob/main/04_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nu4HJeN686y2"
   },
   "source": [
    "# Machine Learning SP 2021/2022\n",
    "\n",
    "Prof. Cesare Alippi<br>\n",
    "Giorgia Adorni ([`giorgia.adorni@usi.ch`](mailto:giorgia.adorni@usi.ch))<br>\n",
    "Luca Butera ([`luca.butera@usi.ch`](mailto:luca.butera@usi.ch))<br>\n",
    "Matteo Riva ([`matteo.riva@usi.ch`](mailto:matteo.riva@usi.ch))\n",
    "\n",
    "---\n",
    "# Lab 04: Feedforward Neural networks\n",
    "\n",
    "\n",
    "Also known as __multilayer perceptrons__ , neural networks are computational models inspired by the connected structure of the brain. The core component of neural networks is the neuron, which is composed of a perceptron and an activation function: \n",
    "\n",
    "$$\n",
    "f(x; \\boldsymbol \\theta) =  h( x^T \\boldsymbol \\theta).\n",
    "$$\n",
    "\n",
    "The main idea behind neural networks is to compose neurons in two different ways: \n",
    "\n",
    "1. by taking many neurons __in parallel__;\n",
    "2. by composing many subsequent __layers__ of neurons;\n",
    "\n",
    "The result is a network of neurons that take data as input, and compute sequential transformations until the desired result is produced as output.\n",
    "\n",
    "![alt text](https://res.cloudinary.com/practicaldev/image/fetch/s--4XiAvCCB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1200/1%2AYgJ6SYO7byjfCmt5uV0PmA.png)\n",
    "\n",
    "---\n",
    "\n",
    "We can write the output of the hidden layer as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "h_0 \\\\\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "\\vdots\\\\\n",
    "h_l \n",
    "\\end{bmatrix}\n",
    "=\n",
    "h\\left(\n",
    "\\begin{bmatrix} \n",
    "w_{00} & w_{01} & w_{02} & \\cdots & w_{0m} \\\\\n",
    "w_{10} & w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
    "w_{20} & w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{l0} & w_{l1} & w_{l2} & \\cdots & w_{lm} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "x_0 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots\\\\\n",
    "x_m\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} \n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots\\\\\n",
    "b_l\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "In short, we write the output of a __layer__ of neurons as:\n",
    "$$\n",
    "H = h(Wx + b_w)\n",
    "$$\n",
    "\n",
    "_NB: without the activation function a layer is a simple affine trasformation._\n",
    "\n",
    "We can compute the output of the network doing the same calculation for the  \"Output\" neurons, with the difference that their input is not $X$, as for the hidden neurons, but it is the output $H$ of the last hidden layer. The output layer can be written as: \n",
    "\n",
    "$$\n",
    "Y = \\sigma(VH + b_v)\n",
    "$$\n",
    "\n",
    "(note that $V$ is a different matrix of parameters).\n",
    "\n",
    "Finally, stacking the two layers simply means __composing__ them together, so that the whole neural network can be written as: \n",
    "\n",
    "$$\n",
    "\\hat y = f(x;\\boldsymbol \\theta = \\{W, b_w, V, b_v\\}) = \\sigma\\left(V h(Wx + b_w)  + b_v\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "Neural networks are trained with __stochastic gradient descent__ (SGD). The key idea behind SGD is to update all the parameters of the network at the same time, based on how each parameter contributed to the __loss__ function $L( \\boldsymbol \\theta)$. \n",
    "\n",
    "The generalized update rule reads: \n",
    "\n",
    "$$\n",
    "{\\boldsymbol \\theta}^{i+1} = {\\boldsymbol \\theta}^{i} + \\varepsilon \\frac{\\partial L({\\boldsymbol \\theta})}{\\partial {\\boldsymbol \\theta}}\\bigg\\vert_{{\\boldsymbol \\theta} = {\\boldsymbol \\theta}^i}\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is again called __learning rate__.\n",
    "\n",
    "---\n",
    "\n",
    "When training neural networks for binary classification, we take the loss to be the __cross-entropy error function__: \n",
    "\n",
    "$$\n",
    "L({\\boldsymbol \\theta}) =  -\\frac1n \\sum_{i=1}^n \\bigg[y_i  \\log \\hat y_i + (1 - y_i)  \\log (1 - \\hat y_i)\\bigg]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dtqk_bGrBFWt"
   },
   "source": [
    "# Neural networks in Python\n",
    "\n",
    "To build our neural network we will use [TensorFlow](https://www.tensorflow.org/), one of the most popular deep learning libraries for Python (the other being [PyTorch](https://pytorch.org/)). \n",
    "TensorFlow provides a huge number of functions, like Numpy, that can be used to manipulate arrays, but offers two great advantages w.r.t. Numpy: \n",
    "\n",
    "1. the computation can be accelerated on GPU via the CUDA library;\n",
    "2. the library implements __automatic differentiation__, meaning that the most analytically complex step of training, the computation of the gradient, is handled for you.\n",
    "\n",
    "While TensorFlow is a very powerful library that offers a fine-coarsened control over what you build, for this course we will skip the low level details and instead use the official high-level API for TensorFlow: [Keras](https://keras.io).\n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "![alt text](https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png)\n",
    "\n",
    "\n",
    "\n",
    "Keras offers collections of TF operations already arranged to implement neural networks with little to no effort. \n",
    "For instance, building a layer of 4 neurons like the one we saw above is as easy as calling `Dense(4)`. That's it. \n",
    "\n",
    "Moreover, Keras offers a high-level API for doing all the usual steps that we usually do when training a neural network, like training on some data, evaluating the performance, and predicting on unseen data. \n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the `Sequential` model, a linear stack of layers.\n",
    "\n",
    "---\n",
    "\n",
    "Let's start with a toy classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZV_0HClD8RL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# color_maps\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF']) \n",
    "\n",
    "# function to generate classification problems\n",
    "def get_data(n, ctype='simple'):\n",
    "  if ctype == 'simple':\n",
    "    x, y = make_classification(n_features=2, \n",
    "                               n_redundant=0, \n",
    "                               n_informative=2, \n",
    "                               n_clusters_per_class=1)\n",
    "    x += np.random.uniform(size=x.shape) # add some noise\n",
    "  elif ctype == 'circles':\n",
    "    x, y = make_circles(n, noise=0.1, factor=0.5)\n",
    "  \n",
    "  elif ctype == 'moons':\n",
    "    x, y = make_moons(n, noise=0.1)\n",
    "  else:\n",
    "    raise ValueError\n",
    "  return x, y.reshape(-1, 1)\n",
    "\n",
    "# function to plot decision boundaries\n",
    "def plot_decision_surface(model, x, y, transform=lambda x:x):    \n",
    "  #init figure\n",
    "  fig = plt.figure()\n",
    "\n",
    "  # Create mesh\n",
    "  h = .01  # step size in the mesh\n",
    "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
    "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
    "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "\n",
    "  # plot train data\n",
    "  plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright,\n",
    "              edgecolors='k')\n",
    "  plt.xlim(xx.min(), xx.max())\n",
    "  plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "  plt.xlabel(r'$x_1$')\n",
    "  plt.ylabel(r'$x_2$');\n",
    "\n",
    "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "\n",
    "  y_pred = y_pred.reshape(xx.shape)\n",
    "  plt.contourf(xx, yy, y_pred > 0.5, cmap=cm, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EHEeg7OEish"
   },
   "source": [
    "Let's go back to the problem that we saw in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsAqR4isEg1I"
   },
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "# Create a classification problem\n",
    "x, y = get_data(120, 'circles')\n",
    "\n",
    "# Let's look at the data\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztKgiKWEFvM-"
   },
   "source": [
    "Now let's build a neural network to fit the data.\n",
    "\n",
    "Using Keras, this will take only a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO_1bKUQExEn"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(25) # this makes the experiment easy to reproduce\n",
    "\n",
    "# Define the network\n",
    "classifier = Sequential()\n",
    "classifier.add(Dense(8, activation='tanh', input_shape=(x.shape[1],)))\n",
    "classifier.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Set up the model for training\n",
    "classifier.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.025), # choose optimizer and learning rate\n",
    "                   loss=losses.BinaryCrossentropy(),                        # define loss function \n",
    "                   metrics=['accuracy']                                     # define metric to monitor during training\n",
    "                  )\n",
    "\n",
    "# Evaluate the performance\n",
    "classifier.fit(x, y, epochs=1000, verbose=0)\n",
    "plot_decision_surface(classifier, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpxTMEB3OHyb"
   },
   "source": [
    "Let's try to understand why this is working adding another hidden layer with 2 units to visualize the space in which a neural network projects the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L93NLuvfCav"
   },
   "outputs": [],
   "source": [
    "def compute_boundary(out_layer):\n",
    "  w = np.asarray(out_layer.weights[0]).ravel()\n",
    "  b = np.asarray(out_layer.bias).ravel()\n",
    "  theta = np.r_[b, w].ravel()\n",
    "  b = -theta[0]/theta[2]\n",
    "  m = -theta[1]/theta[2]\n",
    "\n",
    "  x1 = np.array([x[:,0].min(), x[:,0].max()])\n",
    "  x2 = b + m * x1\n",
    "  return x1, x2\n",
    "\n",
    "\n",
    "\n",
    "def plot_training_iterations(classifier, features, out_layer, iterations=200):\n",
    "  from IPython import display\n",
    "\n",
    "  phi = features.predict(x)\n",
    "  xd, yd = compute_boundary(out_layer)\n",
    "\n",
    "\n",
    "  #init figure\n",
    "  fig = plt.figure()\n",
    "  ax = fig.gca()\n",
    "  #fixed plots\n",
    "  ax.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k', alpha=0.05)\n",
    "  splt = ax.scatter(phi[:, 0], phi[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
    "  line = ax.plot(xd, yd, color='black')[0]\n",
    "  ax.set_xlabel(r'$\\phi_1$')\n",
    "  ax.set_ylabel(r'$\\phi_2$')\n",
    "  ax.set_xlim([-1.3, 1.3])\n",
    "  ax.set_ylim([-1.3, 1.3])\n",
    "  display.display(plt.gcf(), display_id=40)\n",
    "\n",
    "  n = iterations\n",
    "  e = 10\n",
    "  for i in range(n):\n",
    "    hist = classifier.fit(x, y, epochs=e, verbose=0)\n",
    "    phi = features.predict(x)\n",
    "    _, yd = compute_boundary(out_layer)\n",
    "    ax.set_title(f\"Iteration {(i+1)*e}/{e*n} | Train accuracy: {hist.history['accuracy'][-1]:.2f}\")\n",
    "    # update plot\n",
    "    splt.set_offsets(phi)\n",
    "    line.set_ydata(yd)\n",
    "    display.update_display(plt.gcf(), display_id=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZM0dtHVa9gk"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(25)\n",
    "# An alternative way to define a network in keras as a sequence of operations\n",
    "\n",
    "inp = Input((x.shape[1],)) # inpute layer\n",
    "hidden1 = Dense(8, activation='tanh')(inp) # first nonlinear transormation\n",
    "hidden2 = Dense(2, activation='tanh')(hidden1) # second nonlinear transformation\n",
    "\n",
    "out_layer = Dense(1, activation='sigmoid')\n",
    "out = out_layer(hidden2) # output layer\n",
    "\n",
    "# define the model using the input and output layers\n",
    "classifier = Model(inp, out)\n",
    "classifier.compile(loss=losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "features = Model(inp, hidden2)\n",
    "\n",
    "plot_training_iterations(classifier, features, out_layer, iterations=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZnj2c9tZbQ-"
   },
   "source": [
    "The data are linearly separable in the projected space!\n",
    "\n",
    "If you go back and remove the nonlinear activation functions from the hidden layers, you'll see that this is not true anymore. In fact, without nonlinearities the hidden layers are simple affine transormations (e.g., can represent only linear mappings like rotation, translation, shear, ...).\n",
    "\n",
    "_Homework: check [this](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/) insightful blog post from Chris Olah._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYLH_4gCEoDy"
   },
   "outputs": [],
   "source": [
    "features = Model(inp, hidden2)\n",
    "phi = features.predict(x)\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap=cm_bright, edgecolors='k', alpha=0.05)\n",
    "plt.scatter(phi[:, 0], phi[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n",
    "plt.xlabel(r'$\\phi_1$')\n",
    "plt.ylabel(r'$\\phi_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ3Da3hk7bJ9"
   },
   "source": [
    "# Wine quality dataset\n",
    "\n",
    "Let's try with a real dataset now. \n",
    "\n",
    "We are given a set of wine reviews, with the following characteristics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDM0mA0J7ork"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(url, delimiter=';')\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh9yoq9b8bo1"
   },
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the reviews\n",
    "data['quality'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVkInghm-LIV"
   },
   "source": [
    "We can turn this into a binary classification problem by setting a threshold on the reviews: was the wine good (>= 6) or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrUH22Z87x6k"
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = data[data.columns[:-1]].values\n",
    "\n",
    "# Extact targets\n",
    "quality = data['quality'].values.astype(np.int32)\n",
    "y = (quality >= 6).astype(np.int32)\n",
    "plt.hist(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufV3VCjvMmcH"
   },
   "source": [
    "Notice how the values of the features are not commensurable with one another. For instance, \"total sulfur dioxide\" can have values up to 100, while the \"density\" is necessarily limited to be <= 1. \n",
    "\n",
    "While this in principle is not a problem for our machine learning models, in practice it can lead to issues in the training procedure.\n",
    "\n",
    "To standardize the data, we compute the following transformation: \n",
    "\n",
    "$$\n",
    "X_{\\textrm{standardized}} = \\frac{X - \\textrm{mean}(X)}{\\textrm{std}(X)}\n",
    "$$\n",
    "\n",
    "NB: here we are scaling the complete dataset at once for semplicity, but in reality you should use only training data to compute mean and std deviation. Do it in the proper way in the assignments :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cuPF31kMn3t"
   },
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "X -= np.mean(X, axis=0)\n",
    "X /= np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCfbPIHWzWEG"
   },
   "source": [
    "In order to train our network, we will split the data into train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl3yxnz3zsbl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train / test / validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkjyJB0Jz5Yt"
   },
   "source": [
    "Now that we have loaded and pre-processed our data, we only need to build the neural network that we will train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jtB0WD-9W9S"
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "network = Sequential()\n",
    "network.add(Dense(32, activation='relu', input_shape=X.shape[1:]))\n",
    "network.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Prepare the computational graph and training operations\n",
    "network.compile(optimizer='sgd', \n",
    "                loss='binary_crossentropy', \n",
    "                metrics=['acc'])\n",
    "\n",
    "# Train the network\n",
    "network.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the performance\n",
    "eval_results = network.evaluate(X_test, y_test)\n",
    "print('Test loss: {} - Test acc: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd_1f5SYYU0l"
   },
   "source": [
    "# Pokémon dataset: unbalanced classes\n",
    "\n",
    "Pokémon are fictional creatures that are central to the Pokémon franchise.\n",
    "Among them, Legendary Pokémon are a group of incredibly rare and often very powerful Pokémon, generally featured prominently in the legends and myths of the Pokémon world [[source]](https://bulbapedia.bulbagarden.net/wiki/Legendary_Pok%C3%A9mon).\n",
    "\n",
    "The task that we will tackle in this exercise is simple: can we tell whether a Pokémon is legendary or not, by looking at its statistics (like attack, defense, HP, etc.)?\n",
    "\n",
    "Let's start by getting the data and looking at it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFwSoyj7JCd3"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/lgreski/pokemonData/565a330aa57d1f60e1cab9d40320cf7473be566c/Pokemon.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TleWcXz1x-T9"
   },
   "source": [
    "We will train a neural network to predict the \"Legendary\" labels using 'HP', 'Attack', 'Defense', 'SpecialAtk', 'SpecialDef', and 'Speed' as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho6ZJdfHx9Hz"
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "X = data[['HP', 'Attack', 'Defense', 'SpecialAtk', 'SpecialDef', 'Speed']].values\n",
    "\n",
    "# Extact targets\n",
    "y = data['Legendary'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7mZIFyeKZUX"
   },
   "source": [
    "Like we did before, we will need to standardize the data in order to have commensurable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3om98xQpxQsV"
   },
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "X = (X - X.mean(0)) / X.std(0)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfONS-2ixPKn"
   },
   "source": [
    "\n",
    "\n",
    "However, here we face a problem that we didn't have before: we have substantially less samples of one class w.r.t. the other. This means that our neural network is likely to ignore samples with $y=1$, because getting right the samples for which $y=0$ will lead to a lower error. \n",
    "\n",
    "Would you study for an exam question that was only asked once by the professor, in previous years? Or would you focus on the more common exercises that are more likely to be asked again? :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vD2yowI0_OW5"
   },
   "outputs": [],
   "source": [
    "# Plot histogram of labels\n",
    "plt.hist(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDhH9MR_Ktl9"
   },
   "source": [
    "To deal with the __class unbalance__ we will use a simple trick, that will allow our model to learn better. \n",
    "\n",
    "The trick consists in __re-weighting__ the loss function, so that the error on rare samples will count more than the error on common samples:\n",
    "\n",
    "$$\n",
    "L_{\\textrm{reweighted}}(y, f(X; W)) =\n",
    "\\begin{cases}\n",
    "\\lambda_0 L(y, f(X; W))\\textrm{, if } y=0 \\\\\n",
    "\\lambda_1 L(y, f(X; W))\\textrm{, if } y=1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Ideally, $\\lambda_0$ and $\\lambda_1$should represent how rare the respective classes are in the dataset. \n",
    "A common way of computing the two values automatically is as: \n",
    "\n",
    "$$\n",
    "\\lambda_i = \\frac{\\textrm{# samples in dataset}}{\\textrm{# classes}\\cdot\\textrm{# samples of class } i}\n",
    "$$\n",
    "\n",
    "In Keras (and also in Scikit-learn) we call these values `class_weight`.\n",
    "\n",
    "Let's see how to compute them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLklbJ2wKsku"
   },
   "outputs": [],
   "source": [
    "# Split train / test / validation data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
    "\n",
    "# Compute class weights\n",
    "n_pokemons = X_train.shape[0]\n",
    "n_legendaries = y_train.sum()\n",
    "n_classes = 2\n",
    "class_weights = {0: n_pokemons / (n_classes * (n_pokemons - n_legendaries)),\n",
    "                 1: n_pokemons / (n_classes * n_legendaries)}\n",
    "\n",
    "print('Training data: {} legendaries out of {} mons'.format(int(n_legendaries), int(n_pokemons)))\n",
    "print('Training data: class weights {}'.format(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOMlcPfPNHRd"
   },
   "source": [
    "In order to train a neural network in Keras using class weights, we only need to apport some minor modifications to the previous model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6X4xnhZAfAX"
   },
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Dense(32, activation='relu', input_shape=X.shape[1:]))\n",
    "network.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "network.compile('sgd', 'binary_crossentropy', weighted_metrics=['acc'])\n",
    "\n",
    "network.fit(X, y, epochs=25, validation_data=(X_val, y_val))\n",
    "\n",
    "# network.fit(X, y, epochs=100, validation_data=(X_val, y_val), class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7ANYSbXNSch"
   },
   "source": [
    "Finally, let's analyze the __test__ performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4xGKU4tsfWy"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.grid(None)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title='Confusion matrix',\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j],\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "eval_results = network.evaluate(X_test, y_test, verbose=False)\n",
    "print('Loss: {:.4f} - Acc: {:.2f}'.format(*eval_results))\n",
    "\n",
    "y_pred = network.predict(X_test)\n",
    "y_pred = np.round(y_pred)\n",
    "plot_confusion_matrix(y_test, y_pred, classes=['normal', 'legendary']); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hae514a3fKwy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_neural_networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
