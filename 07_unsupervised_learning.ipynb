{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ascarrambad/ml-21-22/blob/main/07_unsupervised_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK6oJGO3smnq"
   },
   "source": [
    "# Machine Learning SP 2021/2022\n",
    "\n",
    "Prof. Cesare Alippi<br>\n",
    "Giorgia Adorni ([`giorgia.adorni@usi.ch`](mailto:giorgia.adorni@usi.ch))<br>\n",
    "Luca Butera ([`luca.butera@usi.ch`](mailto:luca.butera@usi.ch))<br>\n",
    "Matteo Riva ([`matteo.riva@usi.ch`](mailto:matteo.riva@usi.ch))\n",
    "\n",
    "---\n",
    "# Lab 07: Unsupervised learning\n",
    "\n",
    "In this lab, we will see practical applications of unsupervised learning techniques. \n",
    "\n",
    "We will focus on two main tasks: \n",
    "\n",
    "1. Clustering;\n",
    "3. Dimensionality reduction.\n",
    "\n",
    "We will use two datasets that we are now very familiar with:\n",
    " - [Iris](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris)\n",
    " - [MNIST](https://keras.io/api/datasets/mnist/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJl2L7sNQ-zX"
   },
   "source": [
    "\n",
    "## 7.1 Clustering the Iris dataset\n",
    "\n",
    "---\n",
    "\n",
    "In this task, we analyze the Iris dataset by considering **only the features**, without the targets (i.e., the classes). Let's start by loading the dataset and getting a sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi5yKDwV8Sqn"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load the data\n",
    "iris = load_iris()\n",
    "\n",
    "# list the keys\n",
    "print(list(iris.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNwmlzDz27Wz"
   },
   "outputs": [],
   "source": [
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5RucxZp06yS"
   },
   "outputs": [],
   "source": [
    "# read the keys\n",
    "print('feature_names:\\n', iris['feature_names'])\n",
    "print()\n",
    "print('target_names:\\n', iris['target_names'])\n",
    "print()\n",
    "print('data:\\n', iris['data'][:10])\n",
    "print()\n",
    "print('target:\\n', iris['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHj7hStgKPH9"
   },
   "outputs": [],
   "source": [
    "# extract data\n",
    "X = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2jzbohUKT2F"
   },
   "source": [
    "**Remark:** This should be an **unsupervised** learning setup. So, even though `iris.target` is present, we assume to have **no label** associated with the data.\n",
    "\n",
    "Now let's see the shapes of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOcsH8-csmMt"
   },
   "outputs": [],
   "source": [
    "print('Shape of X:', X.shape)\n",
    "\n",
    "(n, d) = X.shape\n",
    "print('d:', d)\n",
    "print('n:', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95GxDfJb7yUj"
   },
   "source": [
    "### 7.1.1 Data visualization\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHAavOb7-UlW"
   },
   "source": [
    "#### Histograms\n",
    "\n",
    "Let's see the estimated pdf of each component (i.e., feature) by means of the histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPXuT8k471qN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "\n",
    "for i in range(d):\n",
    "    # a subplot for each feature\n",
    "    plt.subplot(1, d, i+1)\n",
    "\n",
    "    # histogram\n",
    "    plt.hist(X[:, i], density=True, color=f'C{i}')\n",
    "\n",
    "    # axis labels\n",
    "    plt.xlabel('$x_{}$: {}'.format(i, iris.feature_names[i]))\n",
    "    if i == 0:  plt.ylabel('estimated pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIzyTUHM9Ls"
   },
   "source": [
    "A couple of observations:\n",
    "\n",
    "* the different ranges\n",
    "* $x_2$ and $x_3$ are roughly bimodal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYBLsHDH6sFQ"
   },
   "source": [
    "\n",
    "#### Scatter plots \n",
    "\n",
    "We try to plot more features at the same time. We have 4 features but we can visualize at most 3D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHZvlAeD1cAQ"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "# x0, x1, x2\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_zlabel('$x_2$')\n",
    "\n",
    "# x0, x2, x3\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 2], X[:, 3])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfIOdPHy68sv"
   },
   "source": [
    "It seems that we can identify in a easier way clusters by watching $x_0, x_2$ and $x_3$ jointly instead of $x_0, x_1$ and $x_2$. Can't we?\n",
    "Or maybe it just depends on where we are looking at the data from..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lD2yrpkPKYb"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "# x0, x1, x2\n",
    "ax = fig.add_subplot(121, projection='3d', elev=-150, azim=110)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_zlabel('$x_2$')\n",
    "\n",
    "# x0, x2, x3\n",
    "ax = fig.add_subplot(122, projection='3d', elev=-150, azim=110)\n",
    "ax.scatter(X[:, 0], X[:, 2], X[:, 3])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPjfnHqLPRDo"
   },
   "source": [
    "Without the right perspective we may miss important clues.\n",
    "\n",
    "2D plots are usually clearer than 3D ones (personal opinion!), let's try with them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmlNJSjX8cJv"
   },
   "outputs": [],
   "source": [
    "def plot_every_pair(X, colors=None, same_axis=False, label_pfx=\"x\"):\n",
    "    d = X.shape[1]\n",
    "    if colors is None:\n",
    "        colors = np.zeros(X.shape[0])\n",
    "    n_plots = d*(d-1)//2\n",
    "    plt.figure(figsize=(3 * max(2, n_plots), 8))\n",
    "    ct = 0 \n",
    "    for i in range(1, d+1):\n",
    "        for j in range(i+1, d+1):\n",
    "            ct += 1\n",
    "            plt.subplot(2, np.ceil(n_plots/2), ct)\n",
    "            plt.scatter(X[:, i-1], X[:, j-1], c=colors)\n",
    "            plt.xlabel('${}_{}$'.format(label_pfx, i-1))\n",
    "            plt.ylabel('${}_{}$'.format(label_pfx, j-1))\n",
    "            if same_axis:\n",
    "                # Use same axis scaling\n",
    "                plt.xlim([X.min(), X.max()])\n",
    "                plt.ylim([X.min(), X.max()])\n",
    "    plt.show()\n",
    "\n",
    "plot_every_pair(X)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2YvOsyVQII6"
   },
   "source": [
    "Be careful about the different ranges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBnwPRvkQNEb"
   },
   "outputs": [],
   "source": [
    "plot_every_pair(X, same_axis=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JpGgWB3QBF4"
   },
   "source": [
    "#### Seaborn\n",
    "\n",
    "A cool package for data visualization is `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edcIRy8fPlak"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozdm9rms-vsQ"
   },
   "source": [
    "The above visualization is rather difficult when the number of feature is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I70KdBun9jmp"
   },
   "source": [
    "### 7.1.2 Principal Component Analysis\n",
    "\n",
    "Recall the steps\n",
    "\n",
    "1. Let $X \\in \\mathbb{R}^{n \\times d}$ be the dataset\n",
    "1. Subtract the mean. Should we rescale?\n",
    "  $$ X \\leftarrow X - \\overline{X} $$\n",
    "1. Consider the sample covariance matrix\n",
    "  $$\\hat{\\Sigma} = \\frac{1}{n-1} X^\\top X$$\n",
    "1. Compute the symmetrical and semidefinite positive\n",
    "  $$H = X^\\top X$$\n",
    "  and its eigen-decomposition\n",
    "  $$ H = U \\Lambda U^\\top $$\n",
    "  where $U \\in \\mathbb{R}^{d \\times d}$ is the eigenvectors matrix and $\\Lambda \\in \\mathbb{R}^{d \\times d}$ is the eigenvalues matrix (diagonal).\n",
    "\n",
    "  **Remark 1:** Eigenvalues and eigenvectors: $H \\mathbf u = \\lambda \\mathbf u$\n",
    "\n",
    "1. Now apply the transformation\n",
    "  1. Lossless: apply $U^\\top \\mathbf x$ to each vector (simple rotation).\n",
    "  2. Lossy:\n",
    "    - Discard $l$ eigenvectors obtaining $\\tilde{U} \\in \\mathbb{R}^{d \\times d-l}$.\n",
    "    - apply transformation $\\tilde U^\\top \\mathbf x$ to each vector.\n",
    "\n",
    "  To transform the entire dataset, simply do $XU$ or $X\\tilde U$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDQf58ENAxuv"
   },
   "outputs": [],
   "source": [
    "X_mean = np.mean(X, axis=0, keepdims=True)\n",
    "X0 = X - X_mean\n",
    "\n",
    "H = (X0.T).dot(X0)\n",
    "lam, U = np.linalg.eigh(H)\n",
    "\n",
    "print(\"shapes:\", lam.shape, U.shape)\n",
    "print(\"eigenvalues:\", lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoTyknqWFMVN"
   },
   "source": [
    "We need to reverse `lam` and `U` since we want the eigenvalues to be sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SB4Ohwt9-YRN"
   },
   "outputs": [],
   "source": [
    "# Sort the eigenvalues\n",
    "lam = lam[::-1]\n",
    "U = U[:, ::-1]\n",
    "\n",
    "plt.plot(lam, 'o-')\n",
    "plt.title(\"eigenvalues\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVozxQ8FBcD1"
   },
   "outputs": [],
   "source": [
    "# Apply rotation\n",
    "X_rot = X0.dot(U)\n",
    "\n",
    "plt.figure(figsize=(18, 4))\n",
    "for i in range(d):\n",
    "    plt.subplot(1, d, i+1)\n",
    "    plt.hist(X_rot[:, i])\n",
    "    plt.xlabel('$pc_{}$'.format(i))\n",
    "\n",
    "plot_every_pair(X_rot, same_axis=True, label_pfx=\"pc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRLx6rsl7BlE"
   },
   "outputs": [],
   "source": [
    "# Apply reduced transformation\n",
    "l = 2  # columns to discard\n",
    "Utilde = U[:, :d-l]\n",
    "X_red = X0.dot(Utilde)\n",
    "# Equivalent to X_red = X_rot[:, :d-l]\n",
    "\n",
    "plot_every_pair(X_red, same_axis=True, label_pfx=\"pc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPF8hWfsyxiH"
   },
   "source": [
    "As usual, `sklearn` can speed up our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSBmDO-a8pSt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PCA with sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# d:    num of original features (= num of all principal components)\n",
    "# l:    num of discarded principal components\n",
    "# d-l:  num of considered principal components\n",
    "pca = PCA(n_components=d-l)\n",
    "pca.fit(X0)\n",
    "X_red = pca.transform(X0)\n",
    "# X_red[:, 0] = -X_red[:, 0]  # reverse dimension zero since corresponding eigenvector is symmetric wrt our solution \n",
    "\n",
    "plot_every_pair(X_red, same_axis=True, label_pfx=\"pc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfYSb1UB6RZ-"
   },
   "source": [
    "### 7.1.3 Data reconstruction\n",
    "\n",
    "What about if we want to come back to the original number of components?\n",
    "\n",
    "$$\\mathbf x \\to \\mathbf {\\tilde x} \\to \\mathbf x_{rec} \\approx \\mathbf x$$\n",
    "\n",
    "where $\\mathbf x \\in \\mathbb{R}^{d}, \\mathbf {\\tilde x} \\in \\mathbb{R}^{d-l}$ and $\\mathbf x_{rec} \\in \\mathbb{R}^{d}$. In this way we are able to **compress** the original data in a low dimensional space and restore them (in a **lossy** way!).\n",
    "\n",
    "- Transformation: $\\mathbf{\\tilde x}=\\tilde U^\\top \\mathbf x$.\n",
    "- Reconstruction (inverse transformation): $\\mathbf x_{rec} = \\tilde U \\mathbf{\\tilde x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QnA1N7VUyBT_"
   },
   "outputs": [],
   "source": [
    "# Visualize original vs reconstructed dataset\n",
    "fig = plt.figure(figsize=(18, 4))\n",
    "fig.subplots_adjust(wspace=.4)\n",
    "\n",
    "# Original dataset\n",
    "ax = fig.add_subplot(131, projection='3d', elev=30, azim=160)\n",
    "\n",
    "ax.scatter(X0[:, 0], X0[:, 2], X0[:, 3]) #, X[:, 3])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "ax.set_title(\"$X$\")\n",
    "\n",
    "# Principal components\n",
    "ax = fig.add_subplot(132)\n",
    "ax.scatter(X_red[:, 0], X_red[:, 1])\n",
    "ax.set_xlabel('$pc_0$')\n",
    "ax.set_ylabel('$pc_1$')\n",
    "ax.set_title(\"Principal Components\")\n",
    "ax.axis(\"equal\")\n",
    "\n",
    "# Reconstructed dataset\n",
    "ax = fig.add_subplot(133, projection='3d', elev=30, azim=160)\n",
    "# reconstruct\n",
    "X_rec = pca.inverse_transform(X_red)\n",
    "# which is equivalent to \n",
    "# X_red_ = X.dot(Utilde)\n",
    "# X_rec_ = X_red_.dot(Utilde.T)\n",
    "\n",
    "ax.scatter(X_rec[:, 0], X_rec[:, 2], X_rec[:, 3]) #, X_rec[:, 3])\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$x_3$')\n",
    "ax.set_title(\"$X$ reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbwfV_zUJQK7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(X, columns=iris.feature_names))\n",
    "\n",
    "sns.pairplot(pd.DataFrame(X_red, columns=[f'$pc_{pc}$' for pc in range(d-l)]))\n",
    "\n",
    "sns.pairplot(pd.DataFrame(X_rec, columns=iris.feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmvk-zM9Tx6y"
   },
   "source": [
    "### 7.1.4 Clustering: k-means\n",
    "\n",
    "Now that we managed to represent the original dataset in a low dimensional space we can use the $k$-means clustering technique to classify the data into groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZSFEHNK6sjc"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_clusters = 2\n",
    "\n",
    "k_means = KMeans(n_clusters=k_clusters)\n",
    "cluster_label = k_means.fit_predict(X_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEBoYXMX7bHG"
   },
   "outputs": [],
   "source": [
    "# 3d\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(121, projection='3d', elev=30, azim=160)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_label)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_zlabel('$x_2$')\n",
    "# 2d PC\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(X_red[:, 0], X_red[:, 1], c=cluster_label)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPbfT-xz7n4A"
   },
   "source": [
    "Since we know that there are three classes in `iris.target`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyRzLgYTDEj7"
   },
   "outputs": [],
   "source": [
    "# 3d\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = fig.add_subplot(121, projection='3d', elev=30, azim=160)\n",
    "for c, label in enumerate(iris.target_names):\n",
    "  idxs = iris.target == c\n",
    "  ax.scatter(X[idxs, 0], X[idxs, 1], X[idxs, 2], label=label)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.set_zlabel('$x_2$')\n",
    "# 2d PC\n",
    "ax = fig.add_subplot(122)\n",
    "for c, label in enumerate(iris.target_names):\n",
    "  idxs = iris.target == c\n",
    "  ax.scatter(X_red[idxs, 0], X_red[idxs, 1], label=label)\n",
    "ax.set_xlabel('$x_0$')\n",
    "ax.set_ylabel('$x_1$')\n",
    "ax.axis(\"equal\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Classes (not clusters!)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px4OtYmw6UsO"
   },
   "source": [
    "However, k-means (as well as any other clustering method) does not necessarily retrieve the same classes, because classes are not necessarily confined into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUuGPCB_6TqT"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_clusters = 3\n",
    "\n",
    "k_means = KMeans(n_clusters=k_clusters)\n",
    "cluster_label = k_means.fit_predict(X_red)\n",
    "\n",
    "# 3d\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "ax = fig.add_subplot(131, projection='3d', elev=30, azim=160)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_label)\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_zlabel(r'$x_2$')\n",
    "# 2d PC\n",
    "ax = fig.add_subplot(132)\n",
    "ax.scatter(X_red[:, 0], X_red[:, 1], c=cluster_label)\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.axis(\"equal\")\n",
    "ax.set_title(\"clusters\")\n",
    "\n",
    "# classes\n",
    "ax = fig.add_subplot(133)\n",
    "ax.scatter(X_red[:, 0], X_red[:, 1], c=iris.target)\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.axis(\"equal\")\n",
    "ax.set_title(\"classes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGlmm-BfwRPI"
   },
   "source": [
    "#### Final considerations\n",
    "\n",
    "- We can cross-validate the number of clusters ([silhouette](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html))\n",
    "- Variety of clustering methods with different behaviours ([comparison](https://scikit-learn.org/stable/modules/clustering.html#clustering)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTWiyb1b9-Mt"
   },
   "source": [
    "## 7.2 Image compression\n",
    "\n",
    "---\n",
    "\n",
    "In this part, we will see how we can compress an image dataset, reducing the number of components needed to represent each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "is1xPVeW-ELd"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "dataset = 'mnist'  # 'cifar10'\n",
    "\n",
    "def plot_sample(imgs, labels, nrows, ncols, resize=None, tograyscale=False, shuffle=True):\n",
    "    # create a grid of images\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "    # take a random sample of images\n",
    "    if shuffle:\n",
    "        indices = np.random.choice(len(imgs), size=nrows*ncols, replace=False)\n",
    "    else:\n",
    "        indices = np.arange(nrows*ncols)\n",
    "    for ax, idx in zip(axs.reshape(-1), indices):\n",
    "        ax.axis('off')\n",
    "        # sample an image\n",
    "        ax.set_title(labels[idx])\n",
    "        im = imgs[idx]\n",
    "        if isinstance(im, np.ndarray):\n",
    "            im = Image.fromarray(im)  \n",
    "        if resize is not None:\n",
    "            im = im.resize(resize)\n",
    "        if tograyscale:\n",
    "            im = im.convert('L')\n",
    "        ax.imshow(im, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Load the data\n",
    "if dataset == 'mnist':\n",
    "  from tensorflow.keras.datasets import mnist\n",
    "  (x_train, y_train), _ = mnist.load_data()\n",
    "elif dataset == 'cifar10':\n",
    "  from tensorflow.keras.datasets import mnist, cifar10\n",
    "  (x_train, y_train), _ = cifar10.load_data()\n",
    "  (x_train, y_train) = (x_train.mean(-1), y_train.mean(-1))  # grayscale\n",
    "\n",
    "plot_sample(x_train, y_train, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfH5vrGYNZWo"
   },
   "source": [
    "Vectorize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0gdkrdK-PX0"
   },
   "outputs": [],
   "source": [
    "n_samples = 60000\n",
    "x_train, y_train = x_train[:n_samples], y_train[:n_samples]\n",
    "print(\"x_train:\", x_train.shape) \n",
    "\n",
    "# Reshape to vectors and rescale to [0, 1]\n",
    "w, h = x_train.shape[1:3]\n",
    "X = x_train.reshape(-1, w * h) /255.\n",
    "print(\"X:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNbUIuyFdF5h"
   },
   "source": [
    "Let's plot the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJHcd8K9-a2Z"
   },
   "outputs": [],
   "source": [
    "# PCA\n",
    "n_components = 300\n",
    "\n",
    "X_mean = X.mean(axis=0, keepdims=True)\n",
    "X0 = X - X_mean \n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('eigenvalues')\n",
    "plt.plot(pca.singular_values_**2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVAEH-x8Hyy7"
   },
   "outputs": [],
   "source": [
    "# compress\n",
    "X_red = pca.transform(X0)\n",
    "# extract\n",
    "X_rec = pca.inverse_transform(X_red)\n",
    "X_rec += X_mean\n",
    "\n",
    "# reshape to image size and range\n",
    "x_image_rec = 255 * X_rec.clip(0, 1).reshape(-1, w, h)\n",
    "x_image_orig = 255 * X.clip(0, 1).reshape(-1, w, h)\n",
    "\n",
    "# draw some random images\n",
    "p = np.random.choice(X.shape[0], size=5)\n",
    "print(\"Original images\")\n",
    "plot_sample(x_image_orig[p], y_train[p], 1, 5, shuffle=False)\n",
    "print(\"Reconstructed images\")\n",
    "plot_sample(x_image_rec[p], y_train[p], 1, 5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47By6mUp47MO"
   },
   "source": [
    "### Image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr-KjJWldjEj"
   },
   "source": [
    "It seems that we are able to decently rebuild the original images using way less features! Now what happens if we add noise to the original images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoioU09-_0lz"
   },
   "outputs": [],
   "source": [
    "# Add noise to X\n",
    "X_noisy = X + np.random.randn(*X.shape)*.2\n",
    "\n",
    "# PCA\n",
    "n_components = 50\n",
    "X_mean = X_noisy.mean(axis=0, keepdims=True)\n",
    "X0 = X_noisy - X_mean \n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X0)\n",
    "\n",
    "plt.figure()\n",
    "plt.title('eigenvalues')\n",
    "plt.plot(pca.singular_values_**2)\n",
    "plt.show()\n",
    "\n",
    "# compress\n",
    "X_red = pca.transform(X0)\n",
    "# extract\n",
    "X_rec = pca.inverse_transform(X_red)\n",
    "X_rec += X_mean\n",
    "\n",
    "# reshape to image size and range\n",
    "x_image_orig = 255 * X.clip(0, 1).reshape(-1, w, h)\n",
    "x_image_noisy = 255 * X_noisy.clip(0, 1).reshape(-1, w, h)\n",
    "x_image_rec = 255 * X_rec.clip(0, 1).reshape(-1, w, h)\n",
    "\n",
    "# draw some random images\n",
    "p = np.random.choice(X.shape[0], size=5)\n",
    "print(\"Original images\")\n",
    "plot_sample(x_image_orig[p], y_train[p], 1, 5, shuffle=False)\n",
    "print(\"Noisy images\")\n",
    "plot_sample(x_image_noisy[p], y_train[p], 1, 5, shuffle=False)\n",
    "print(\"Reconstructed images\")\n",
    "plot_sample(x_image_rec[p], y_train[p], 1, 5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkOfev2beS23"
   },
   "source": [
    "Now let's see if using only two dimensions we are able to plot the dataset in a clusterized fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgKekvc7QYD_"
   },
   "outputs": [],
   "source": [
    "x_dim, y_dim = 0, 1\n",
    "plt.figure(figsize=(16, 12))\n",
    "for d in range(10):\n",
    "    ii = np.where(y_train==d)[0]\n",
    "    plt.scatter(X_red[ii][:, x_dim], X_red[ii][:, y_dim], marker=f\"${d}$\", label=d)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "fJl2L7sNQ-zX"
   ],
   "include_colab_link": true,
   "name": "07_unsupervised_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
